# ç¬¬ 8 ç«  Wake-Word Detection ãƒ¡ãƒ¢

## ã“ã®ç« ã®ç›®çš„

- ã‚¦ã‚§ã‚¤ã‚¯ãƒ¯ãƒ¼ãƒ‰ï¼ˆéŸ³å£°ãƒˆãƒªã‚¬ãƒ¼ï¼‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç†è§£ã™ã‚‹
- éŸ³å£°ã‚³ãƒãƒ³ãƒ‰ã‚’å­¦ç¿’ã•ã›ã‚‹

## ã‚­ãƒ¼ã‚¢ã‚¤ãƒ‡ã‚¢



## ã‚„ã£ã¦ã¿ãŸã“ã¨

- [] æœ¬ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’ PC ä¸Šã§å‹•ã‹ã™

## ç–‘å•ãƒ»ãƒ¡ãƒ¢

- ç’°å¢ƒå†æ§‹ç¯‰ï¼š
1. å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆUbuntu/Debianï¼‰
```bash
sudo apt update
sudo apt install -y make build-essential libssl-dev zlib1g-dev \
  libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \
  libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev \
  liblzma-dev git
```

2. pyenv ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
curl https://pyenv.run | bash
# ã‚·ã‚§ãƒ«åˆæœŸåŒ–ï¼ˆbash ã®ä¾‹ï¼‰
echo 'export PYENV_ROOT="$HOME/.pyenv"' >> ~/.bashrc
echo 'export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.bashrc
echo 'eval "$(pyenv init --path)"' >> ~/.bashrc
echo 'eval "$(pyenv init -)"' >> ~/.bashrc
# æ–°ã—ã„ã‚·ã‚§ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã‹ä»¥ä¸‹ã‚’å®Ÿè¡Œ
source ~/.bashrc
```

3. Python 3.10 ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã§ä½¿ã†
```bash
pyenv install 3.10.16      # ã¾ãŸã¯å¥½ããª 3.10.x
cd /home/agake/work/Oreilly_TinyML
pyenv local 3.10.16        # ã“ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã§ Python 3.10 ã‚’ä½¿ã†
```

4. ä»®æƒ³ç’°å¢ƒä½œæˆãƒ»æœ‰åŠ¹åŒ–ãƒ»ä¾å­˜ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip setuptools
pip install -r requirements.txt
```


## ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ãƒ¢ãƒ‡ãƒ«ä½œæˆ
- /content ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹ã®ã§ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
```bash
mkdir -p ~/work/Oreilly_TinyML/content
sudo ln -s ~/work/Oreilly_TinyML/content /content
```

- TesorFlow<=1.15ãŒå¿…è¦ãªã®ã§ã€python3.7ã®ç’°å¢ƒã‚’dockerã§ä½œæˆã€ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚¦ãƒ³ãƒˆã—ã¦ã‚³ãƒ³ãƒ†ãƒŠã§ä½œæ¥­
```bash
docker run -it --rm -v "$PWD":/workspace -w /workspace tensorflow/tensorflow:1.15.5-py3 bash
```

- dockerã‚³ãƒ³ãƒ†ãƒŠã®ç’°å¢ƒã§notebookwpå®Ÿè¡Œã™ã‚‹
ãƒ›ã‚¹ãƒˆOSã§
```bash
cd /home/agake/work/Oreilly_TinyML

docker run -it --rm -p 8888:8888 \
-v "/home/agake/work/Oreilly_TinyML":/workspace \
-v "/home/agake/work/src/tensorflow":/workspace/tensorflow \
-w /workspace \
tensorflow/tensorflow:1.15.5-py3
bash -c "apt-get update -qq && apt-get install -y -qq xxd && \
pip install -q notebook tf-estimator-nightly==1.14.0.dev2019072901 && \
mkdir -p /workspace/content && (ln -s /workspace/content /content || true) && \
jupyter notebook --ip=0.0.0.0 --no-browser --allow-root --notebook-dir=/workspace"
```
ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹
http://127.0.0.1:8888/?token=<tokennnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn>

`!python tensorflow/tensorflow/examples/speech_commands/...`
ã‚’
`!python /workspace/tensorflow/tensorflow/examples/speech_commands/...`
ã«ä¿®æ­£ã—ã¦å®Ÿè¡Œ

ğŸ˜²Convert the modelã§AbortãŒãŠãã‚‹
ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®é‡ã¿ãŒNaNï¼ˆãŠã‚ˆã³é‡å­åŒ–ç”¨ min/max ãŒ 0/NaNï¼‰ã«ãªã£ã¦ã„ã‚‹ãŸã‚ã€freezeâ†’toco ã®é‡å­åŒ–ã§ scale=0/NaN ã«ãªã‚Š abort ã—ã¦ã„ã¾ã™ã€‚
å¯¾å‡¦ã¯ã€Œã¾ãšé‡å­åŒ–ã‚’ã‚ªãƒ•ã«ã—ã¦å®‰å®šã—ãŸ float ãƒ¢ãƒ‡ãƒ«ã‚’å‡ºã™ â†’ å¿…è¦ãªã‚‰å¾Œå‡¦ç†é‡å­åŒ–ï¼ˆä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã§ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã€ã§ã™ã€‚
1. å­¦ç¿’è¨­å®šã‚’å¤‰æ›´ã—ã¦å†å­¦ç¿’ï¼ˆã¾ãš --quantize=0ã€å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹ï¼‰
- ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã® env ã‚’å¤‰æ›´ï¼ˆä¾‹ï¼‰:
```
os.environ["TRAINING_STEPS"]="15000,3000"
os.environ["LEARNING_RATE"]="0.0005,0.0001"
```
- å†å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ï¼ˆã‚³ãƒ³ãƒ†ãƒŠå†…ã§å®Ÿè¡Œï¼‰:
```
python /workspace/tensorflow/tensorflow/examples/speech_commands/train.py \
  --model_architecture=tiny_conv --window_stride=20 --preprocess=micro \
  --wanted_words=${WANTED_WORDS} --silence_percentage=25 --unknown_percentage=25 \
  --quantize=0 --verbosity=WARN --how_many_training_steps=${TRAINING_STEPS} \
  --learning_rate=${LEARNING_RATE} --summaries_dir=/content/retrain_logs \
  --data_dir=/content/speech_dataset --train_dir=/content/speech_commands_train
```

2. å­¦ç¿’ä¸­ã« NaN ãŒå‡ºã¦ã„ãªã„ã‹ãƒ­ã‚°/TensorBoard ã‚’ç¢ºèªï¼ˆ/content/retrain_logs ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰ã€‚
3. å­¦ç¿’å¾Œã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å†æ¤œæŸ»ï¼ˆNaN ãŒç„¡ã„ã“ã¨ã‚’ç¢ºèªï¼‰:
```
python - <<'PY'
import os, numpy as np, tensorflow as tf
s=os.environ.get('TOTAL_STEPS','18000')
ckpt=f"/content/speech_commands_train/tiny_conv.ckpt-{s}"
r=tf.train.NewCheckpointReader(ckpt)
for n in sorted(r.get_variable_to_shape_map().keys()):
    v=r.get_tensor(n); print(n, np.nanmin(v), np.nanmax(v), np.isnan(v).sum())
PY
```

4. float ã§ freeze â†’ tflite ã«å¤‰æ›ï¼ˆã¾ãšå‹•ä½œç¢ºèªï¼‰:
```
python /workspace/tensorflow/tensorflow/examples/speech_commands/freeze.py \
  --model_architecture=tiny_conv --window_stride=20 --preprocess=micro \
  --wanted_words=${WANTED_WORDS} --quantize=0 --output_file=/content/tiny_conv_float.pb \
  --start_checkpoint=/content/speech_commands_train/tiny_conv.ckpt-${TOTAL_STEPS}

!toco \
 --graph_def_file=/content/tiny_conv_float.pb --output_file=/content/tiny_conv_float.tflite \
 --input_shapes=1,49,40,1 --input_arrays=Reshape_2 --output_arrays=labels_softmax \
 --inference_type=FLOAT
```

5. å¿…è¦ãªã‚‰å¾Œå‡¦ç†é‡å­åŒ–ï¼ˆä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ã€Python API æ¨å¥¨ï¼‰:
- Floatã¸å¤‰æ›
```shell
python - <<'PY'
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_frozen_graph(
  '/content/tiny_conv_float.pb',
  input_arrays=['Reshape_2'],
  output_arrays=['labels_softmax'],
  input_shapes={'Reshape_2':[1,49,40,1]}
)
tflite_model = converter.convert()
open('/content/tiny_conv_float.tflite','wb').write(tflite_model)
print('wrote /content/tiny_conv_float.tflite')
PY
ls -lh /content/tiny_conv_float.tflite
```

- é‡å­åŒ–-1 ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ
 AudioProcessor ãŒãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ä½¿ã£ã¦ã„ã‚‹ã€Œå‰å‡¦ç†ã€ã‚’æä¾›ã—ã¾ã™ã€‚ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã¯å­¦ç¿’æ™‚ã¨åŒã˜å‰å‡¦ç†ã‚’é€šã—ãŸå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ã¯ validation/testing ã®ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’ä½¿ã†ã®ãŒæœ€è‰¯ã§ã™ã€‚ä¸‹è¨˜ã¯ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ãã®ã¾ã¾å‹•ãã€AudioProcessor.get_data() ã‚’ä½¿ã£ãŸä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä»˜ãã®ä¾‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ï¼ˆmicro / mfcc ç­‰ã€å­¦ç¿’æ™‚ã® model_settings ã«åˆã‚ã›ã¦å‹•ãã¾ã™ï¼‰ã€‚

å­¦ç¿’ã§ä½¿ã£ãŸå‰å‡¦ç†ã‚’å¿…ãšä½¿ã†ï¼ˆã“ã“ã§ã¯ AudioProcessor ã®å‡¦ç†ï¼‰
ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã¯ augmentation ã‚’å…¥ã‚Œãªã„ï¼ˆbackground_volume=0, time_shift=0ï¼‰ã‹ã¤ mode='validation'/'testing' ã‚’ä½¿ã†
100ã€œ500 ã‚µãƒ³ãƒ—ãƒ«ç¨‹åº¦ã§ååˆ†ï¼ˆ100 æ¨å¥¨ï¼‰
å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚³ãƒ³ãƒ†ãƒŠå†…ã§å®Ÿè¡Œï¼‰:
```shell
python - <<'PY'
import sys
sys.path.insert(0, '/workspace/tensorflow/tensorflow/examples/speech_commands')

import os, numpy as np, tensorflow as tf
from tensorflow.python.platform import gfile
import input_data, models

# ç’°å¢ƒå¤‰æ•° / ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¨åŒã˜è¨­å®šã‚’ä½¿ã£ã¦ãã ã•ã„
WANTED_WORDS = os.environ.get('WANTED_WORDS','yes,no')
DATA_DIR = '/content/speech_dataset'
SUMMARIES_DIR = '/content/retrain_logs'
TRAIN_DIR = '/content/speech_commands_train'

# ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ train ã¨åŒã˜ã«ä½œã‚‹
model_settings = models.prepare_model_settings(
    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),
    sample_rate=16000, clip_duration_ms=1000,
    window_size_ms=30.0, window_stride_ms=20.0,  # â† ã“ã“ã‚’ 20.0 ã«
    feature_bin_count=40, preprocess='micro'
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œã£ã¦ AudioProcessor åˆæœŸåŒ–ï¼ˆdata_dir ãŒæ—¢ã«ã‚ã‚‹æƒ³å®šï¼‰
sess = tf.compat.v1.InteractiveSession()
audio_processor = input_data.AudioProcessor(
    None, DATA_DIR, 25, 25, WANTED_WORDS.split(','), 10, 10,
    model_settings, SUMMARIES_DIR)

# helper: fingerprint shape
fingerprint_size = model_settings['fingerprint_size']
fingerprint_width = model_settings['fingerprint_width']
frames = fingerprint_size // fingerprint_width  # ã“ã‚ŒãŒ 49 ã«ãªã‚‹ã¯ãš

# ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼ˆvalidation ã‚»ãƒƒãƒˆã‹ã‚‰å…ˆé ­ N ã‚’ä½¿ã†ï¼‰
N = 200
def representative_gen():
    for i in range(N):
        data, labels = audio_processor.get_data(
            1, i, model_settings, background_frequency=0.0,
            background_volume_range=0.0, time_shift=0, mode='validation', sess=sess)
        # data ã¯ (1, fingerprint_size) flattened
        # representative_gen ã® reshape ã‚‚ (1,49,40,1) ã«åˆã‚ã›ã‚‹
        arr = data.reshape(1, frames, fingerprint_width, 1).astype(np.float32)
        yield [arr]

converter = tf.lite.TFLiteConverter.from_frozen_graph(
    '/content/tiny_conv_float.pb',
    input_arrays=['Reshape_2'],
    output_arrays=['labels_softmax'],
    input_shapes={'Reshape_2':[1,frames,fingerprint_width,1]}
)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_gen
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_quant = converter.convert()
open('/content/tiny_conv_quant_from_audio.tflite','wb').write(tflite_quant)
print('wrote /content/tiny_conv_quant_from_audio.tflite')
PY
ls -lh /content/tiny_conv_quant_from_audio.tflite
```


- å†å­¦ç¿’å®Ÿè¡Œ
```shell
export WANTED_WORDS="yes,no"
export TRAINING_STEPS="15000,3000"
export LEARNING_RATE="0.0005,0.0001"
python /workspace/tensorflow/tensorflow/examples/speech_commands/train.py \
  --model_architecture=tiny_conv --window_stride=20 --preprocess=micro \
  --wanted_words=${WANTED_WORDS} --silence_percentage=25 --unknown_percentage=25 \
  --quantize=0 --verbosity=WARN --how_many_training_steps=${TRAINING_STEPS} \
  --learning_rate=${LEARNING_RATE} --summaries_dir='' \
  --data_dir=/content/speech_dataset --train_dir=/content/speech_commands_train \
  --check_nans=True
  ```

